{
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "name": "intro_md"
   },
   "source": [
    "# Fleet Analytics with Iceberg V3\n",
    "\n",
    "This notebook is the **live demo companion** to the Iceberg V3 Comprehensive Guide. It contains all the hands-on exercises so you can follow along without switching between browser tabs.\n",
    "\n",
    "## What You'll Explore\n",
    "\n",
    "1. **Streaming Ingestion**: Real-time vehicle telemetry to VARIANT in Iceberg via Snowpipe Streaming\n",
    "2. **Web API Ingestion**: Pull weather data from Open-Meteo API into VARIANT in Iceberg\n",
    "3. **Batch JSON Ingestion**: Load maintenance logs with VARIANT to Iceberg\n",
    "4. **Dynamic Tables**: Declarative transformation pipelines on Iceberg\n",
    "5. **Security & Governance**: Sensitive data classification, Tagging, masking policies, data quality monitoring, and lineage on Iceberg tables\n",
    "6. **Analytics**: Time-series and geospatial analytics with Iceberg V3 data types\n",
    "7. **AI**: AI agents on top of Iceberg tables\n",
    "8. **Business Continuity and Disaster Recovery**: Incremental cross-cloud and region replication with failover for Iceberg tables\n",
    "9. **Interoperability**: Interoperable access from other engines with centralized fine-grained access controls for Apache Spark\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook:\n",
    "1. \u2705 Run `setup.sh` to create all database objects\n",
    "2. \u2705 Start the streaming script: `python stream_telemetry.py`\n",
    "3. \u2705 Enable **OPEN_METEO_ACCESS** external integration (for API section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "language": "sql",
    "name": "set_context_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Set the context\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "USE DATABASE FLEET_ANALYTICS_DB;\n",
    "USE SCHEMA RAW;\n",
    "USE WAREHOUSE FLEET_ANALYTICS_WH;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "verify_setup_md"
   },
   "source": [
    "## Verify Setup\n",
    "\n",
    "Before proceeding, let's verify all objects were created correctly by the setup script."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "verify_iceberg_tables_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Verify all Iceberg tables exist\n",
    "SHOW ICEBERG TABLES IN DATABASE FLEET_ANALYTICS_DB;\n",
    "\n",
    "-- Should show: VEHICLE_TELEMETRY_STREAM, MAINTENANCE_LOGS, SENSOR_READINGS, \n",
    "-- VEHICLE_LOCATIONS, VEHICLE_REGISTRY, API_WEATHER_DATA"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "verify_dynamic_tables_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Verify Dynamic Iceberg Tables\n",
    "SHOW DYNAMIC TABLES IN DATABASE FLEET_ANALYTICS_DB;\n",
    "\n",
    "-- Should show: TELEMETRY_ENRICHED, DAILY_FLEET_SUMMARY"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "verify_governance_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Verify governance objects\n",
    "SHOW MASKING POLICIES IN DATABASE FLEET_ANALYTICS_DB;\n",
    "SHOW DATA METRIC FUNCTIONS IN DATABASE FLEET_ANALYTICS_DB;\n",
    "\n",
    "-- Verify files are staged\n",
    "LIST @RAW.LOGS_STAGE;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000006",
   "metadata": {
    "name": "streaming_intro_md"
   },
   "source": [
    "## Section 1: Streaming and Ingestion\n",
    "\n",
    "The Python streaming script (`stream_telemetry.py`) simulates a fleet of 50 vehicles sending real-time telemetry events. Each event contains:\n",
    "\n",
    "- **Location**: GPS coordinates with lat/lon\n",
    "- **Speed**: Current vehicle speed in MPH\n",
    "- **Engine metrics**: Temperature, oil pressure, fuel level\n",
    "- **Diagnostics**: Warning indicators and error codes\n",
    "- **Driver behavior**: Hard braking, acceleration events\n",
    "\n",
    "The data is stored in the **VEHICLE_TELEMETRY_STREAM** Iceberg table using the **VARIANT** data type for flexible schema.\n",
    "\n",
    "### Start the Streaming Script (Terminal)\n",
    "\n",
    "If not already running, open a terminal and run:\n",
    "```bash\n",
    "cd iceberg-v3-tables-comprehensive-guide/assets\n",
    "source iceberg_v3_demo_venv/bin/activate\n",
    "python stream_telemetry.py\n",
    "```\n",
    "\n",
    "> **Connection Issues?** If you're on a VPN, you may need a network policy. Set `ENABLE_NETWORK_POLICY=true` in config.env and re-run setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000007",
   "metadata": {
    "language": "sql",
    "name": "streaming_count_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Check current count of streaming events\n",
    "SELECT COUNT(*) AS event_count FROM VEHICLE_TELEMETRY_STREAM;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000008",
   "metadata": {
    "language": "sql",
    "name": "streaming_view_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- View the latest streaming events with VARIANT extraction\n",
    "SELECT \n",
    "    VEHICLE_ID,\n",
    "    EVENT_TIMESTAMP,\n",
    "    TELEMETRY_DATA:location:lat::FLOAT AS latitude,\n",
    "    TELEMETRY_DATA:location:lon::FLOAT AS longitude,\n",
    "    TELEMETRY_DATA:speed_mph::FLOAT AS speed_mph,\n",
    "    TELEMETRY_DATA:engine:temperature_f::INT AS engine_temp,\n",
    "    TELEMETRY_DATA:engine:fuel_level_pct::FLOAT AS fuel_level\n",
    "FROM VEHICLE_TELEMETRY_STREAM\n",
    "ORDER BY EVENT_TIMESTAMP DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "name": "weather_api_intro_md"
   },
   "source": [
    "### Pull Data from Web APIs\n",
    "\n",
    "Snowflake can connect directly to web APIs and ingest semi-structured responses into Iceberg tables with VARIANT.\n",
    "\n",
    "**\u26a0\ufe0f Before running the Python cell below:**\n",
    "1. Click the **Notebook settings** (gear icon, top right)\n",
    "2. Select **External access**\n",
    "3. Toggle ON **OPEN_METEO_ACCESS** integration\n",
    "4. Click **Save**\n",
    "\n",
    "This allows the notebook to access the Open-Meteo weather API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000010",
   "metadata": {
    "language": "sql",
    "name": "weather_describe_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- The table was created during setup\n",
    "DESCRIBE TABLE API_WEATHER_DATA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "weather_verify_empty",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Confirm it's empty\n",
    "SELECT COUNT(*) AS current_count FROM API_WEATHER_DATA;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "language": "python",
    "name": "weather_fetch_python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "# Fetch weather data from Open-Meteo API and load into Iceberg table\n",
    "# NOTE: Ensure OPEN_METEO_ACCESS external integration is enabled in notebook settings\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.functions import col, parse_json, current_timestamp\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "# Define cities for fleet operations\n",
    "fleet_cities = [\n",
    "    {\"name\": \"Los Angeles\", \"lat\": 34.0522, \"lon\": -118.2437},\n",
    "    {\"name\": \"Seattle\", \"lat\": 47.6062, \"lon\": -122.3321},\n",
    "    {\"name\": \"Denver\", \"lat\": 39.7392, \"lon\": -104.9903},\n",
    "    {\"name\": \"Chicago\", \"lat\": 41.8781, \"lon\": -87.6298}\n",
    "]\n",
    "\n",
    "weather_records = []\n",
    "for city in fleet_cities:\n",
    "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": city[\"lat\"],\n",
    "        \"longitude\": city[\"lon\"],\n",
    "        \"current\": \"temperature_2m,wind_speed_10m,precipitation\",\n",
    "        \"hourly\": \"temperature_2m,precipitation_probability\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        weather_records.append({\n",
    "            \"CITY_NAME\": city[\"name\"],\n",
    "            \"LATITUDE\": city[\"lat\"],\n",
    "            \"LONGITUDE\": city[\"lon\"],\n",
    "            \"API_RESPONSE\": response.text\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Failed to fetch weather for {city['name']}: {response.status_code}\")\n",
    "\n",
    "# Create DataFrame and write to Iceberg table\n",
    "if weather_records:\n",
    "    df = session.create_dataframe(weather_records)\n",
    "    df = df.with_column(\"WEATHER_DATA\", parse_json(col(\"API_RESPONSE\")))\n",
    "    df.select(\n",
    "        col(\"CITY_NAME\"),\n",
    "        col(\"LATITUDE\"),\n",
    "        col(\"LONGITUDE\"),\n",
    "        col(\"WEATHER_DATA\"),\n",
    "        current_timestamp().alias(\"INGESTED_AT\")\n",
    "    ).write.mode(\"append\").save_as_table(\"API_WEATHER_DATA\")\n",
    "    print(f\"Loaded weather data for {len(weather_records)} cities\")\n",
    "else:\n",
    "    print(\"No weather data fetched. Check external access integration is enabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000013",
   "metadata": {
    "language": "sql",
    "name": "weather_query_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- View the loaded weather data\n",
    "SELECT \n",
    "    CITY_NAME,\n",
    "    WEATHER_DATA:current:temperature_2m::FLOAT AS current_temp_c,\n",
    "    WEATHER_DATA:current:wind_speed_10m::FLOAT AS wind_speed_kmh,\n",
    "    WEATHER_DATA:current:precipitation::FLOAT AS precipitation_mm,\n",
    "    INGESTED_AT\n",
    "FROM API_WEATHER_DATA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "weather_query_sql_flatten",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Extract hourly forecast for a city\nSELECT \n    CITY_NAME,\n    f.value::FLOAT AS hourly_temp\nFROM API_WEATHER_DATA,\nLATERAL FLATTEN(input => WEATHER_DATA:hourly:temperature_2m) f\nWHERE CITY_NAME = 'Seattle'\nLIMIT 24;",
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000015",
   "metadata": {
    "name": "batch_json_intro_md",
    "collapsed": false
   },
   "source": [
    "### Batch Ingest JSON Logs\n",
    "\n",
    "Fleet maintenance logs are collected as JSON files and loaded into Snowflake using **COPY INTO**. Each log file contains maintenance events with nested data:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"log_id\": \"LOG-001\",\n",
    "  \"vehicle_id\": \"VH-1234\",\n",
    "  \"event_type\": \"SCHEDULED_MAINTENANCE\",\n",
    "  \"severity\": \"MEDIUM\",\n",
    "  \"parts\": [{\"name\": \"oil_filter\", \"cost\": 25.99}],\n",
    "  \"total_cost\": 175.50\n",
    "}\n",
    "```\n",
    "\n",
    "The entire JSON payload is stored in a **VARIANT** column (`LOG_DATA`) for schema flexibility.\n",
    "\n",
    "#### Sample JSON Files\n",
    "\n",
    "The setup script uploaded 10 sample maintenance log files to the internal stage `@LOGS_STAGE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000016",
   "metadata": {
    "language": "sql",
    "name": "batch_list_stage_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- List the staged JSON files\n",
    "LIST @LOGS_STAGE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000017",
   "metadata": {
    "language": "sql",
    "name": "batch_copy_into_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Load JSON files into Iceberg table\n",
    "COPY INTO MAINTENANCE_LOGS (LOG_ID, VEHICLE_ID, LOG_TIMESTAMP, LOG_DATA, SOURCE_FILE)\n",
    "FROM (\n",
    "    SELECT \n",
    "        $1:log_id::VARCHAR,\n",
    "        $1:vehicle_id::VARCHAR,\n",
    "        $1:log_timestamp::TIMESTAMP_NTZ,\n",
    "        $1,\n",
    "        METADATA$FILENAME\n",
    "    FROM @LOGS_STAGE\n",
    ")\n",
    "FILE_FORMAT = (TYPE = 'JSON' STRIP_OUTER_ARRAY = TRUE)\n",
    "PATTERN = '.*maintenance_log.*\\.json';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000018",
   "metadata": {
    "language": "sql",
    "name": "batch_view_logs_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- View the loaded maintenance logs with VARIANT data\n",
    "SELECT \n",
    "    LOG_ID,\n",
    "    VEHICLE_ID,\n",
    "    LOG_TIMESTAMP,\n",
    "    LOG_DATA:event_type::STRING AS event_type,\n",
    "    LOG_DATA:severity::STRING AS severity,\n",
    "    LOG_DATA:description::STRING AS description,\n",
    "    LOG_DATA:total_cost::FLOAT AS total_cost\n",
    "FROM MAINTENANCE_LOGS\n",
    "ORDER BY LOG_TIMESTAMP DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000019",
   "metadata": {
    "name": "dynamic_tables_intro_md"
   },
   "source": [
    "## Section 2: Declarative Transformation Pipelines\n",
    "\n",
    "Dynamic Iceberg Tables provide **declarative, incremental transformations**. Instead of scheduling ETL jobs, you define the desired output and Snowflake automatically:\n",
    "\n",
    "- \u2705 Tracks source changes\n",
    "- \u2705 Incrementally refreshes data\n",
    "- \u2705 Maintains data freshness within your target lag\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "```\n",
    "VEHICLE_TELEMETRY_STREAM \u2500\u252c\u2500\u2192 TELEMETRY_ENRICHED \u2500\u2192 DAILY_FLEET_SUMMARY\n",
    "                          \u2502         \u2191\n",
    "VEHICLE_REGISTRY \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n",
    "                                    \u2502\n",
    "SENSOR_READINGS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**TELEMETRY_ENRICHED**: Joins streaming telemetry with vehicle registry for context\n",
    "**DAILY_FLEET_SUMMARY**: Aggregates to daily metrics by region and vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000020",
   "metadata": {
    "language": "sql",
    "name": "dynamic_tables_show_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- View all dynamic tables in the pipeline\n",
    "SHOW DYNAMIC TABLES IN DATABASE FLEET_ANALYTICS_DB;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000099",
   "metadata": {
    "name": "dynamic_tables_ui_md"
   },
   "source": [
    "#### Explore the Pipeline in the UI\n",
    "\n",
    "1. Navigate to **Data** \u2192 **Databases** \u2192 **FLEET_ANALYTICS_DB**\n",
    "2. Filter by database: **FLEET_ANALYTICS_DB**\n",
    "3. Select any Dynamic Table (e.g., `TELEMETRY_ENRICHED`)\n",
    "4. Explore the tabs:\n",
    "   - **Graph**: Visualize the full pipeline and dependencies\n",
    "   - **Refresh History**: See incremental refresh operations\n",
    "   - **Definition**: Review the declarative SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000021",
   "metadata": {
    "language": "sql",
    "name": "dynamic_tables_query_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Query enriched telemetry with vehicle details\n",
    "SELECT \n",
    "    VEHICLE_ID,\n",
    "    MAKE,\n",
    "    MODEL,\n",
    "    DRIVER_NAME,\n",
    "    FLEET_REGION,\n",
    "    speed_mph,\n",
    "    engine_temp_f,\n",
    "    engine_health_status,\n",
    "    driving_behavior,\n",
    "    EVENT_TIMESTAMP\n",
    "FROM CURATED.TELEMETRY_ENRICHED\n",
    "WHERE engine_health_status != 'NORMAL' OR driving_behavior = 'AGGRESSIVE'\n",
    "ORDER BY EVENT_TIMESTAMP DESC\n",
    "LIMIT 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000022",
   "metadata": {
    "name": "masking_intro_md"
   },
   "source": [
    "## Section 3: Security, Governance, and BCDR\n",
    "\n",
    "This section highlights Horizon Catalog as the central, unified, interoperable catalog with enterprise-grade security features. All governance capabilities work seamlessly with Iceberg V3 tables.\n",
    "\n",
    "### PII Detection with Classification\n",
    "\n",
    "Snowflake can automatically detect and classify sensitive data across your entire database, including Iceberg tables.\n",
    "\n",
    "#### Classify the Schema\n",
    "\n",
    "1. Navigate to **Data** \u2192 **Databases** \u2192 **FLEET_ANALYTICS_DB** \u2192 **RAW** (the schema)\n",
    "2. Click the **three dots menu** (\u22ee) in the top right corner\n",
    "3. Select **Classify and Tag Sensitive Data**\n",
    "4. Select all Iceberg tables in the schema\n",
    "5. Toggle **Automatically tag data** ON\n",
    "6. Click **Classify and Tag Sensitive Data**\n",
    "\n",
    "#### View Applied Tags\n",
    "\n",
    "After classification completes:\n",
    "\n",
    "1. Navigate to any table (e.g., `VEHICLE_REGISTRY`)\n",
    "2. Click the **Columns** tab\n",
    "3. Notice tags applied to sensitive columns like `DRIVER_NAME`, `DRIVER_EMAIL`, `DRIVER_PHONE`\n",
    "\n",
    "Common tags that may be applied:\n",
    "- `SEMANTIC_CATEGORY:NAME` - Personal names\n",
    "- `SEMANTIC_CATEGORY:EMAIL` - Email addresses\n",
    "- `SEMANTIC_CATEGORY:PHONE_NUMBER` - Phone numbers\n",
    "- `SEMANTIC_CATEGORY:US_SSN` - Social Security Numbers\n",
    "\n",
    "> **Tip**: You can also create [custom tags that automatically propagate](https://docs.snowflake.com/en/user-guide/object-tagging/work#define-a-tag-that-will-automatically-propagate) to existing and new downstream objects.\n",
    "\n",
    "### Fine-Grained Access Control with Masking Policies\n",
    "\n",
    "Masking policies protect sensitive data by dynamically transforming values based on the querying user's role. The **VEHICLE_REGISTRY** table contains driver PII:\n",
    "\n",
    "| Column | Policy | Behavior |\n",
    "|--------|--------|----------|\n",
    "| DRIVER_NAME | PII_NAME_MASK | Admins see full name; analysts see \"Jo****hn\" |\n",
    "| DRIVER_EMAIL | PII_EMAIL_MASK | Admins see full email; analysts see \"jo****@****.com\" |\n",
    "| DRIVER_PHONE | PII_PHONE_MASK | Admins see full phone; analysts see \"+1-555-***-**23\" |\n",
    "\n",
    "#### Apply Masking Policies (UI Method)\n",
    "\n",
    "1. Navigate to **Data** \u2192 **Databases** \u2192 **FLEET_ANALYTICS_DB** \u2192 **RAW** \u2192 **VEHICLE_REGISTRY**\n",
    "2. Click **Columns** tab\n",
    "3. Click **+** next to a column's Policy field\n",
    "4. Select the appropriate mask and click **Done**\n",
    "\n",
    "Below we'll test masking behavior by switching roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000030",
   "metadata": {
    "language": "sql",
    "name": "masking_show_policies_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- View all masking policies in the database\n",
    "SHOW MASKING POLICIES IN DATABASE FLEET_ANALYTICS_DB;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000023",
   "metadata": {
    "language": "sql",
    "name": "masking_admin_query_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- As ACCOUNTADMIN, see full PII values (unmasked)\n",
    "SELECT VEHICLE_ID, DRIVER_NAME, DRIVER_EMAIL, DRIVER_PHONE\n",
    "FROM VEHICLE_REGISTRY LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "masking_create_role_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Create a test analyst role and grant it to the current user\n",
    "CREATE ROLE IF NOT EXISTS FLEET_ANALYST;\n",
    "GRANT USAGE ON DATABASE FLEET_ANALYTICS_DB TO ROLE FLEET_ANALYST;\n",
    "GRANT USAGE ON SCHEMA FLEET_ANALYTICS_DB.RAW TO ROLE FLEET_ANALYST;\n",
    "GRANT SELECT ON ALL TABLES IN SCHEMA FLEET_ANALYTICS_DB.RAW TO ROLE FLEET_ANALYST;\n",
    "GRANT USAGE ON WAREHOUSE FLEET_ANALYTICS_WH TO ROLE FLEET_ANALYST;\n",
    "\n",
    "-- Grant the role to the current user so we can switch to it\n",
    "SET MY_USER = CURRENT_USER();\n",
    "GRANT ROLE FLEET_ANALYST TO USER IDENTIFIER($MY_USER);"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000024"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "masking_analyst_query_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Switch to analyst role and observe MASKED values\n",
    "USE ROLE FLEET_ANALYST;\n",
    "SELECT VEHICLE_ID, DRIVER_NAME, DRIVER_EMAIL, DRIVER_PHONE\n",
    "FROM VEHICLE_REGISTRY LIMIT 5;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000025"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "masking_switch_back_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Switch back to ACCOUNTADMIN for remaining operations\n",
    "USE ROLE ACCOUNTADMIN;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000026"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "data_quality_intro_md"
   },
   "source": [
    "### Data Quality Monitoring\n",
    "\n",
    "Data Metric Functions (DMFs) validate data quality automatically. The Data Quality dashboard will initially be empty - DMFs run on a schedule and results populate after the first run.\n",
    "\n",
    "> **Note**: To see results sooner, set `TRIGGER_ON_CHANGES` and insert data to trigger the DMF."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000027"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "dmf_trigger_locations_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Set DMFs to trigger when data changes\n",
    "ALTER ICEBERG TABLE RAW.VEHICLE_LOCATIONS SET DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES';\n",
    "ALTER ICEBERG TABLE RAW.SENSOR_READINGS SET DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES';"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000028"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "dmf_check_references_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Check which DMFs are attached to VEHICLE_LOCATIONS\n",
    "SELECT * FROM TABLE(INFORMATION_SCHEMA.DATA_METRIC_FUNCTION_REFERENCES(\n",
    "    REF_ENTITY_NAME => 'FLEET_ANALYTICS_DB.RAW.VEHICLE_LOCATIONS',\n",
    "    REF_ENTITY_DOMAIN => 'TABLE'\n",
    "));"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000029"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "dmf_insert_trigger_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Insert a row to trigger the DMF (if using TRIGGER_ON_CHANGES)\n",
    "INSERT INTO RAW.VEHICLE_LOCATIONS (LOCATION_ID, VEHICLE_ID, LOCATION_TIMESTAMP, LATITUDE, LONGITUDE, LOCATION_POINT, SPEED_MPH, HEADING_DEGREES, FLEET_REGION)\n",
    "SELECT UUID_STRING(), VEHICLE_ID, CURRENT_TIMESTAMP(), LATITUDE, LONGITUDE, LOCATION_POINT, SPEED_MPH, HEADING_DEGREES, FLEET_REGION\n",
    "FROM RAW.VEHICLE_LOCATIONS LIMIT 1;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000031"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "dmf_check_results_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Check DMF results (may take a minute to populate after data insert)\n",
    "SELECT \n",
    "    TABLE_NAME,\n",
    "    METRIC_NAME,\n",
    "    VALUE,\n",
    "    MEASUREMENT_TIME\n",
    "FROM SNOWFLAKE.LOCAL.DATA_QUALITY_MONITORING_RESULTS\n",
    "WHERE TABLE_DATABASE = 'FLEET_ANALYTICS_DB'\n",
    "ORDER BY MEASUREMENT_TIME DESC\n",
    "LIMIT 20;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000032"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "data_quality_ui_md"
   },
   "source": [
    "#### View Data Quality in the UI\n",
    "\n",
    "After the DMF has run, view results in Snowsight:\n",
    "\n",
    "1. Navigate to **Data** \u2192 **Databases** \u2192 **FLEET_ANALYTICS_DB** \u2192 **RAW** \u2192 **VEHICLE_LOCATIONS**\n",
    "2. Click the **Data Quality** tab\n",
    "3. View metrics by dimension (Freshness, Volume, Accuracy, etc.)\n",
    "\n",
    "> **Tip**: If the dashboard is still empty, wait 1-2 minutes after inserting data and refresh the page."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000033"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "lineage_md"
   },
   "source": [
    "### Data Lineage\n",
    "\n",
    "Snowflake automatically tracks lineage for all objects, including Iceberg tables and Dynamic Tables.\n",
    "\n",
    "#### View Lineage Graph\n",
    "\n",
    "1. Navigate to **Data** \u2192 **Databases** \u2192 **FLEET_ANALYTICS_DB** \u2192 **RAW** \u2192 **MAINTENANCE_LOGS**\n",
    "2. Click on the **Lineage** tab\n",
    "3. Explore the lineage graph showing both upstream and downstream dependencies, including upstream lineage from an external telemetry messaging service\n",
    "\n",
    "The lineage graph is valuable for:\n",
    "- **Impact analysis**: Understanding what's affected when source data changes\n",
    "- **Data quality investigation**: Tracing issues back to their source\n",
    "- **Security auditing**: Understanding data flow for sensitive information\n",
    "\n",
    "> \ud83d\udca1 **Try it**: Navigate to `VEHICLE_TELEMETRY_STREAM` and view its lineage to see how data flows through `TELEMETRY_ENRICHED` to `DAILY_FLEET_SUMMARY`."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000034"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "bcdr_md"
   },
   "source": [
    "### Business Continuity and Disaster Recovery\n",
    "\n",
    "This section highlights Snowflake's enterprise-grade, out-of-the-box business continuity and disaster recovery capabilities that extend to Iceberg V3 tables.\n",
    "\n",
    "> **Note**: Replication features require Business Critical Edition or higher.\n",
    "\n",
    "#### Iceberg Table Replication Overview\n",
    "\n",
    "Snowflake supports replicating Iceberg tables across regions and clouds, ensuring your data lakehouse is protected against regional outages. See [Iceberg Table Replication documentation](https://docs.snowflake.com/en/user-guide/tables-iceberg-replication) for details.\n",
    "\n",
    "### Setting Up Replication via UI\n",
    "\n",
    "Follow these steps to set up cross-region replication for your Iceberg tables:\n",
    "\n",
    "##### Step 1: Create a Replication Group\n",
    "\n",
    "1. Navigate to **Admin** \u2192 **Accounts**\n",
    "2. Click on the **Replication** tab\n",
    "3. Click **+ Replication Group** (or **+ Failover Group** for automatic failover)\n",
    "4. Configure the group:\n",
    "   - **Name**: `FLEET_ANALYTICS_REPLICATION`\n",
    "   - **Replication Schedule**: Choose your RPO (e.g., every 10 minutes)\n",
    "   - **Objects to replicate**: Select `FLEET_ANALYTICS_DB` database\n",
    "\n",
    "##### Step 2: Select Target Account\n",
    "\n",
    "1. Choose the target Snowflake account in a different region\n",
    "2. If no secondary accounts exist, you'll need to create one first\n",
    "\n",
    "##### Step 3: Configure Objects\n",
    "\n",
    "1. Select all objects to replicate:\n",
    "   - Databases (includes all Iceberg tables)\n",
    "   - Warehouses (optional)\n",
    "   - Roles and privileges (recommended for consistent access)\n",
    "\n",
    "##### Step 4: Enable and Monitor\n",
    "\n",
    "1. Click **Create** to enable replication\n",
    "2. Monitor replication status in the Replication tab\n",
    "3. View replication lag and sync history\n",
    "\n",
    "#### Failover Testing\n",
    "\n",
    "For Failover Groups (automatic failover):\n",
    "\n",
    "1. In the Replication tab, select your failover group\n",
    "2. Click **Failover** to initiate failover to secondary\n",
    "3. Verify applications can connect to secondary account\n",
    "4. Use **Failback** when primary is restored"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000035"
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000036",
   "metadata": {
    "name": "analytics_intro_md"
   },
   "source": [
    "## Section 4: Analytics and AI\n",
    "\n",
    "This section demonstrates Snowflake's analytical capabilities on Iceberg V3 tables, including semi-structured data querying, time-series analysis, geospatial functions, and AI agents.\n",
    "\n",
    "### Semi-Structured Data Analytics\n",
    "\n",
    "Snowflake efficiently queries VARIANT data in Iceberg tables with automatic pruning and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "analytics_variant_sql",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Extract nested fields from VARIANT with dot notation\n",
    "SELECT \n",
    "    VEHICLE_ID,\n",
    "    TELEMETRY_DATA:speed_mph::FLOAT AS speed,\n",
    "    TELEMETRY_DATA:engine:rpm::INT AS engine_rpm,\n",
    "    TELEMETRY_DATA:engine:temperature_f::INT AS engine_temp,\n",
    "    TELEMETRY_DATA:diagnostics:check_engine::BOOLEAN AS check_engine_light,\n",
    "    TELEMETRY_DATA:driver_behavior:hard_brake_count::INT AS hard_brakes\n",
    "FROM RAW.VEHICLE_TELEMETRY_STREAM\n",
    "WHERE TELEMETRY_DATA:speed_mph::FLOAT > 50\n",
    "LIMIT 20;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000037"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "analytics_variant_agg_sql",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Aggregate on variant fields - find vehicles with check engine warnings\n",
    "SELECT \n",
    "    TELEMETRY_DATA:diagnostics:check_engine::BOOLEAN AS check_engine,\n",
    "    COUNT(*) AS event_count,\n",
    "    ROUND(AVG(TELEMETRY_DATA:speed_mph::FLOAT), 1) AS avg_speed,\n",
    "    COUNT(DISTINCT VEHICLE_ID) AS vehicle_count\n",
    "FROM RAW.VEHICLE_TELEMETRY_STREAM\n",
    "GROUP BY 1\n",
    "ORDER BY event_count DESC;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000038"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "analytics_variant_pruning_sql",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Run with profile to see pruning efficiency\n",
    "SELECT \n",
    "    VEHICLE_ID,\n",
    "    EVENT_TIMESTAMP,\n",
    "    TELEMETRY_DATA:speed_mph::FLOAT AS speed\n",
    "FROM VEHICLE_TELEMETRY_STREAM\n",
    "WHERE VEHICLE_ID = 'VH-1234'\n",
    "  AND TELEMETRY_DATA:engine:temperature_f::INT > 200;\n",
    "\n",
    "-- Check query profile in History tab to see partition pruning statistics"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000039"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "analytics_time_series_md"
   },
   "source": [
    "### Time-Series Analytics and Forecasting\n",
    "\n",
    "The `SENSOR_READINGS` table contains high-precision time-series data for analysis.\n",
    "\n",
    "#### AS OF Join for Point-in-Time Analysis\n",
    "Find the closest sensor reading **before** each maintenance event. This is invaluable for:\n",
    "- Correlating maintenance with preceding anomalies\n",
    "- Root cause analysis\n",
    "- Predictive maintenance modeling"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000040"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000041",
   "metadata": {
    "language": "sql",
    "name": "analytics_asof_join_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- AS OF join: Find closest sensor reading before each maintenance event\n",
    "SELECT \n",
    "    m.VEHICLE_ID,\n",
    "    m.LOG_TIMESTAMP AS maintenance_time,\n",
    "    m.LOG_DATA:event_type::STRING AS maintenance_type,\n",
    "    s.READING_TIMESTAMP AS closest_reading_time,\n",
    "    s.ENGINE_TEMP_F,\n",
    "    s.OIL_PRESSURE_PSI\n",
    "FROM RAW.MAINTENANCE_LOGS m\n",
    "ASOF JOIN RAW.SENSOR_READINGS s\n",
    "    MATCH_CONDITION (m.LOG_TIMESTAMP >= s.READING_TIMESTAMP)\n",
    "    ON m.VEHICLE_ID = s.VEHICLE_ID\n",
    "WHERE m.LOG_DATA:severity::STRING IN ('CRITICAL', 'HIGH')\n",
    "ORDER BY m.LOG_TIMESTAMP DESC\n",
    "LIMIT 15;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "analytics_time_series_agg_md"
   },
   "source": [
    "#### Time-Series Aggregation with Windows\n",
    "\n",
    "Window functions enable powerful time-series analysis directly in SQL:\n",
    "- **Rolling averages** to smooth out noise\n",
    "- **Running totals** for cumulative metrics\n",
    "- **Lag/Lead** for period-over-period comparisons"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000042"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "analytics_time_series_window_sql",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Calculate rolling averages for engine temperature\n",
    "SELECT \n",
    "    VEHICLE_ID,\n",
    "    READING_TIMESTAMP,\n",
    "    ENGINE_TEMP_F,\n",
    "    AVG(ENGINE_TEMP_F) OVER (\n",
    "        PARTITION BY VEHICLE_ID \n",
    "        ORDER BY READING_TIMESTAMP \n",
    "        RANGE BETWEEN INTERVAL '1 HOUR' PRECEDING AND CURRENT ROW\n",
    "    ) AS rolling_avg_temp,\n",
    "    MAX(ENGINE_TEMP_F) OVER (\n",
    "        PARTITION BY VEHICLE_ID \n",
    "        ORDER BY READING_TIMESTAMP \n",
    "        RANGE BETWEEN INTERVAL '1 HOUR' PRECEDING AND CURRENT ROW\n",
    "    ) AS rolling_max_temp\n",
    "FROM RAW.SENSOR_READINGS\n",
    "ORDER BY VEHICLE_ID, READING_TIMESTAMP\n",
    "LIMIT 50;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000043"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "ml_forecast_md"
   },
   "source": [
    "#### ML Forecasting for Predictive Maintenance\n",
    "\n",
    "Snowflake's ML.FORECAST can build time-series forecast models directly on Iceberg tables. For Iceberg tables, use `SYSTEM$QUERY_REFERENCE` instead of `TABLE` reference.\n",
    "\n",
    "> \u23f1\ufe0f **Training Time**: Model training may take **2-5 minutes** depending on data volume and warehouse size. To speed up training:\n",
    "> - Use a **larger warehouse** (e.g., MEDIUM or LARGE) - training scales with compute\n",
    "> - Reduce the number of time series by filtering to specific vehicles\n",
    "> - Limit historical data range (e.g., 14 days instead of 30)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000044"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "ml_forecast_train_sql",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Create a forecast model for fuel consumption\n",
    "-- For Iceberg tables, we must use SYSTEM$QUERY_REFERENCE instead of TABLE reference\n",
    "CREATE OR REPLACE SNOWFLAKE.ML.FORECAST fuel_consumption_forecast(\n",
    "    INPUT_DATA => SYSTEM$QUERY_REFERENCE('\n",
    "        SELECT \n",
    "            VEHICLE_ID,\n",
    "            READING_TIMESTAMP,\n",
    "            FUEL_CONSUMPTION_GPH\n",
    "        FROM RAW.SENSOR_READINGS\n",
    "        WHERE READING_TIMESTAMP > DATEADD(''day'', -30, CURRENT_TIMESTAMP())\n",
    "    '),\n",
    "    TIMESTAMP_COLNAME => 'READING_TIMESTAMP',\n",
    "    TARGET_COLNAME => 'FUEL_CONSUMPTION_GPH',\n",
    "    SERIES_COLNAME => 'VEHICLE_ID'\n",
    ");"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000045"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "ml_forecast_inference_sql",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- FORECAST: Generate a 7-day forecast (168 hours) and save to a table for visualization\n",
    "CALL fuel_consumption_forecast!FORECAST(\n",
    "    FORECASTING_PERIODS => 168,\n",
    "    CONFIG_OBJECT => {'prediction_interval': 0.95}\n",
    ");\n",
    "\n",
    "-- Save forecast results to a table for visualization\n",
    "CREATE OR REPLACE TEMPORARY TABLE FUEL_FORECAST_RESULTS AS\n",
    "SELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()));\n",
    "\n",
    "-- Preview forecast results\n",
    "SELECT * FROM FUEL_FORECAST_RESULTS ORDER BY SERIES, TS LIMIT 20;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000046"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "ml_forecast_graph_md"
   },
   "source": [
    "##### Visualize Historical + Forecast Data\n",
    "\n",
    "Use Streamlit in a Snowflake Notebook to plot historical data alongside the forecast:"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000047"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "ml_forecast_graph_py",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# \ud83d\udcc8 Visualize Historical + Forecast Fuel Consumption\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "# Get historical data (last 30 days for one vehicle)\n",
    "historical_df = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        READING_TIMESTAMP AS ts,\n",
    "        FUEL_CONSUMPTION_GPH AS value,\n",
    "        'Historical' AS data_type\n",
    "    FROM RAW.SENSOR_READINGS\n",
    "    WHERE VEHICLE_ID = (SELECT SERIES FROM FUEL_FORECAST_RESULTS LIMIT 1)\n",
    "      AND READING_TIMESTAMP > DATEADD('day', -30, CURRENT_TIMESTAMP())\n",
    "    ORDER BY READING_TIMESTAMP\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "# Get forecast data (column names vary - check with DESCRIBE first if needed)\n",
    "forecast_df = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        TS AS ts,\n",
    "        FORECAST AS value,\n",
    "        'Forecast' AS data_type\n",
    "    FROM FUEL_FORECAST_RESULTS\n",
    "    WHERE SERIES = (SELECT SERIES FROM FUEL_FORECAST_RESULTS LIMIT 1)\n",
    "    ORDER BY TS\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "# Get the vehicle ID for the title\n",
    "vehicle_id = session.sql(\"SELECT SERIES FROM FUEL_FORECAST_RESULTS LIMIT 1\").collect()[0][0]\n",
    "\n",
    "st.subheader(f\"\ud83d\udcc8 Fuel Consumption Forecast: {vehicle_id}\")\n",
    "st.caption(\"Historical data (30 days) + 7-day forecast\")\n",
    "\n",
    "# Normalize column names to lowercase\n",
    "historical_df.columns = historical_df.columns.str.lower()\n",
    "forecast_df.columns = forecast_df.columns.str.lower()\n",
    "\n",
    "# Combine for plotting\n",
    "combined_df = pd.concat([historical_df, forecast_df], ignore_index=True)\n",
    "combined_df['ts'] = pd.to_datetime(combined_df['ts'])\n",
    "combined_df = combined_df.sort_values('ts')\n",
    "\n",
    "# Create the chart using Streamlit's native line chart\n",
    "# Pivot data for multi-line chart\n",
    "chart_data = combined_df.pivot_table(index='ts', columns='data_type', values='value', aggfunc='first')\n",
    "st.line_chart(chart_data, use_container_width=True)\n",
    "\n",
    "# Display metrics\n",
    "col1, col2, col3 = st.columns(3)\n",
    "col1.metric(\"Avg Historical (GPH)\", f\"{historical_df['value'].mean():.2f}\")\n",
    "col2.metric(\"Avg Forecast (GPH)\", f\"{forecast_df['value'].mean():.2f}\")\n",
    "change = ((forecast_df['value'].mean() - historical_df['value'].mean()) / historical_df['value'].mean()) * 100\n",
    "col3.metric(\"Projected Change\", f\"{change:+.1f}%\")\n",
    "\n",
    "st.caption(\"\ud83d\udca1 The forecast helps identify vehicles likely to have higher fuel consumption, enabling proactive maintenance scheduling.\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000048"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "geo_analytics_md"
   },
   "source": [
    "### Geospatial Analytics with H3 Indexing\n",
    "\n",
    "H3 is Uber's hierarchical geospatial indexing system. Snowflake's H3 functions enable:\n",
    "- Aggregating vehicles by geographic cell\n",
    "- Efficient spatial joins and clustering\n",
    "- Multi-resolution analysis (cells at different zoom levels)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000049"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "geo_analytics_1_sql",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Calculate distance between consecutive positions\n-- Note: LAG doesn't support GEOGRAPHY directly, so we use lat/lon coordinates\nSELECT \n    VEHICLE_ID,\n    LOCATION_TIMESTAMP,\n    LATITUDE,\n    LONGITUDE,\n    LAG(LATITUDE) OVER (PARTITION BY VEHICLE_ID ORDER BY LOCATION_TIMESTAMP) AS prev_lat,\n    LAG(LONGITUDE) OVER (PARTITION BY VEHICLE_ID ORDER BY LOCATION_TIMESTAMP) AS prev_lon,\n    ST_DISTANCE(\n        LOCATION_POINT,\n        ST_MAKEPOINT(\n            LAG(LONGITUDE) OVER (PARTITION BY VEHICLE_ID ORDER BY LOCATION_TIMESTAMP),\n            LAG(LATITUDE) OVER (PARTITION BY VEHICLE_ID ORDER BY LOCATION_TIMESTAMP)\n        )\n    ) / 1609.34 AS distance_miles  -- Convert meters to miles\nFROM VEHICLE_LOCATIONS\nORDER BY VEHICLE_ID, LOCATION_TIMESTAMP\nLIMIT 50;",
   "id": "ce110000-1111-2222-3333-ffffff000050"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "geo_analytics_2_sql",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Find vehicles inside a geofence (Los Angeles area)\nWITH geofence AS (\n    SELECT ST_MAKEPOLYGON(TO_GEOGRAPHY(\n        'LINESTRING(-118.7 33.5, -118.7 34.5, -117.7 34.5, -117.7 33.5, -118.7 33.5)'\n    )) AS la_area\n)\nSELECT \n    v.VEHICLE_ID,\n    v.LOCATION_TIMESTAMP,\n    ST_X(v.LOCATION_POINT) AS longitude,\n    ST_Y(v.LOCATION_POINT) AS latitude,\n    v.FLEET_REGION\nFROM VEHICLE_LOCATIONS v, geofence g\nWHERE ST_WITHIN(v.LOCATION_POINT, g.la_area)\nORDER BY v.LOCATION_TIMESTAMP DESC\nLIMIT 20;",
   "id": "ce110000-1111-2222-3333-ffffff000051"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "geo_analytics_3_sql",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Geospatial: Aggregate vehicles by H3 cell (resolution 6)\n",
    "SELECT \n",
    "    H3_POINT_TO_CELL_STRING(LOCATION_POINT, 6) AS h3_cell,\n",
    "    FLEET_REGION,\n",
    "    COUNT(DISTINCT VEHICLE_ID) AS vehicle_count,\n",
    "    ROUND(AVG(SPEED_MPH), 1) AS avg_speed\n",
    "FROM RAW.VEHICLE_LOCATIONS\n",
    "WHERE LOCATION_TIMESTAMP > DATEADD('hour', -24, CURRENT_TIMESTAMP())\n",
    "GROUP BY 1, 2\n",
    "ORDER BY vehicle_count DESC\n",
    "LIMIT 20;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000052"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000053",
   "metadata": {
    "language": "python",
    "name": "analytics_geospatial_py"
   },
   "outputs": [],
   "source": [
    "# Visualize fleet locations on an interactive map using Streamlit\n",
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "# Query location data with H3 cell centers (use ST_X/ST_Y for GEOGRAPHY type)\n",
    "df = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        H3_POINT_TO_CELL_STRING(LOCATION_POINT, 6) AS h3_cell,\n",
    "        ST_X(H3_CELL_TO_POINT(H3_POINT_TO_CELL_STRING(LOCATION_POINT, 6))) AS longitude,\n",
    "        ST_Y(H3_CELL_TO_POINT(H3_POINT_TO_CELL_STRING(LOCATION_POINT, 6))) AS latitude,\n",
    "        FLEET_REGION,\n",
    "        COUNT(DISTINCT VEHICLE_ID) AS vehicle_count,\n",
    "        ROUND(AVG(SPEED_MPH), 1) AS avg_speed\n",
    "    FROM RAW.VEHICLE_LOCATIONS\n",
    "    WHERE LOCATION_TIMESTAMP > DATEADD('hour', -24, CURRENT_TIMESTAMP())\n",
    "    GROUP BY 1, 2, 3, 4\n",
    "    ORDER BY vehicle_count DESC\n",
    "    LIMIT 100\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "st.subheader(\"\ud83d\uddfa\ufe0f Fleet Vehicle Distribution\")\n",
    "st.caption(\"Each point represents an H3 cell with vehicle activity in the last 24 hours\")\n",
    "\n",
    "# Display metrics\n",
    "col1, col2, col3 = st.columns(3)\n",
    "col1.metric(\"Total H3 Cells\", len(df))\n",
    "col2.metric(\"Total Vehicles\", int(df['VEHICLE_COUNT'].sum()))\n",
    "col3.metric(\"Avg Speed\", f\"{df['AVG_SPEED'].mean():.1f} mph\")\n",
    "\n",
    "# Interactive map visualization - point size scales with vehicle count\n",
    "st.map(df, latitude='LATITUDE', longitude='LONGITUDE', size='VEHICLE_COUNT')\n",
    "\n",
    "# Show data table\n",
    "st.dataframe(df[['FLEET_REGION', 'VEHICLE_COUNT', 'AVG_SPEED']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "ai_agent_md"
   },
   "source": [
    "### AI Agents with Snowflake Intelligence\n",
    "\n",
    "The setup script creates a Cortex Agent (`FLEET_ANALYTICS_AGENT`) that enables natural language querying of your fleet data.\n",
    "\n",
    "> **Note**: Cortex Agents require Enterprise Edition or higher.\n",
    "\n",
    "#### Region Availability and Cross-Region Inference\n",
    "\n",
    "Cortex Agents and Cortex Analyst rely on LLMs that may not be available in all Snowflake regions. If you see an error like:\n",
    "\n",
    "```\n",
    "None of the preferred models are authorized or available in your region...\n",
    "```\n",
    "\n",
    "You have two options:\n",
    "\n",
    "1. **Enable Cross-Region Inference** (recommended): This allows Snowflake to route AI requests to regions where models are available, while keeping your data secure."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000054"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "ai_agent_setup_sql",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Enable cross-region inference for your account\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "ALTER ACCOUNT SET CORTEX_ENABLED_CROSS_REGION = 'ANY_REGION';"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000055"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "ai_agent_setup_md"
   },
   "source": [
    "2. **Use a supported region**: Deploy your Snowflake account in a region with native LLM support.\n",
    "\n",
    "For more details, see:\n",
    "- [Cortex Analyst Region Availability](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-analyst#region-availability)\n",
    "- [Cross-Region Inference](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cross-region-inference)\n",
    "\n",
    "> **Privacy Note**: With cross-region inference, only the AI inference is routed cross-region. Your underlying data remains in your account's region and is protected by Snowflake's governance controls."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000056"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "create_ai_agent_md"
   },
   "source": [
    "#### Using the Fleet Analytics Agent\n",
    "\n",
    "1. Navigate to **Cortex AI** \u2192 **Agents** in the left sidebar\n",
    "2. Select **FLEET_ANALYTICS_AGENT**\n",
    "3. Start asking questions in natural language:\n",
    "\n",
    "**Example queries:**\n",
    "\n",
    "- \"Which vehicles had the highest fuel consumption last week?\"\n",
    "- \"Show me all critical maintenance events\"\n",
    "- \"What's the average speed by fleet region?\"\n",
    "- \"Find vehicles with check engine warnings\"\n",
    "- \"Which drivers have the most hard braking events?\"\n",
    "- \"Show vehicle health scores below 60\"\n",
    "- \"How many vehicles are in each H3 cell in California?\"\n",
    "\n",
    "The agent understands your Iceberg table schema including:\n",
    "- **VARIANT columns**: Extracts nested JSON fields using colon notation\n",
    "- **GEOGRAPHY columns**: Uses H3 and ST_* functions for geospatial analysis\n",
    "- **Time-series data**: Performs ASOF joins and window functions\n",
    "\n",
    "#### Agent Configuration\n",
    "\n",
    "The agent was created with access to these tables:\n",
    "\n",
    "| Table | Description |\n",
    "|-------|-------------|\n",
    "| `RAW.VEHICLE_TELEMETRY_STREAM` | Real-time streaming telemetry (VARIANT) |\n",
    "| `RAW.VEHICLE_LOCATIONS` | Geospatial positions (GEOGRAPHY) |\n",
    "| `RAW.SENSOR_READINGS` | High-precision time-series sensor data |\n",
    "| `RAW.MAINTENANCE_LOGS` | Maintenance events (VARIANT) |\n",
    "| `RAW.VEHICLE_REGISTRY` | Vehicle and driver master data |\n",
    "| `ANALYTICS.DAILY_FLEET_SUMMARY` | Daily aggregated metrics |\n",
    "| `ANALYTICS.VEHICLE_HEALTH_SCORE` | Calculated health scores |\n",
    "\n",
    "To view or modify the agent configuration:"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000057"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "interop_md"
   },
   "source": [
    "## Section 5: Interoperability\n",
    "\n",
    "This section demonstrates that external engines like Apache Spark can access Iceberg V3 tables managed by Snowflake. Horizon provides temporary, scoped storage credentials and enforces centralized row/column-level access controls for any engine.\n",
    "\n",
    "### Prerequisites for Spark\n",
    "\n",
    "Ensure you have:\n",
    "- Conda installed\n",
    "- The repository cloned (`sfguide-iceberg-v3-comprehensive`)\n",
    "\n",
    "### Start the Spark Environment\n",
    "\n",
    "The setup script created a Conda environment with Spark 4.0+ and all necessary dependencies:\n",
    "\n",
    "```bash\n",
    "cd iceberg-v3-tables-comprehensive-guide/assets\n",
    "\n",
    "# Activate the Spark environment\n",
    "conda activate fleet-spark\n",
    "\n",
    "# Start Jupyter notebook\n",
    "jupyter notebook notebooks/spark_iceberg_interop.ipynb\n",
    "```\n",
    "\n",
    "With `spark_iceberg_interop.ipynb` open in your browser, follow the instructions in the notebook."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000058"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cleanup_md"
   },
   "source": [
    "## Cleanup\n",
    "Run the cell below to remove all objects created by this guide."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000059"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cleanup_sql",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "-- Drop the database (this removes all tables, schemas, and objects)\n",
    "DROP DATABASE IF EXISTS FLEET_ANALYTICS_DB;\n",
    "\n",
    "-- Drop the external volume (if you created one)\n",
    "DROP EXTERNAL VOLUME IF EXISTS FLEET_ICEBERG_VOL;\n",
    "\n",
    "-- Drop the warehouse\n",
    "DROP WAREHOUSE IF EXISTS FLEET_ANALYTICS_WH;\n",
    "\n",
    "-- Drop roles\n",
    "DROP ROLE IF EXISTS FLEET_ANALYST;\n",
    "DROP ROLE IF EXISTS FLEET_ENGINEER;\n",
    "DROP ROLE IF EXISTS FLEET_ADMIN;\n",
    "\n",
    "-- If you created replication groups, remove them\n",
    "DROP REPLICATION GROUP IF EXISTS FLEET_ANALYTICS_REPLICATION;\n",
    "\n",
    "-- Remove network policy from user (if applied)\n",
    "ALTER USER <your_username> UNSET NETWORK_POLICY;\n",
    "DROP NETWORK POLICY IF EXISTS FLEET_STREAMING_POLICY;\n",
    "\n",
    "-- External access integration\n",
    "DROP INTEGRATION IF EXISTS OPEN_METEO_ACCESS;\n",
    "\n",
    "SELECT 'Uncomment the commands above to cleanup' AS NOTE;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000060"
  }
 ]
}